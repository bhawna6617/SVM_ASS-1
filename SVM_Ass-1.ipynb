{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1bc0144",
   "metadata": {},
   "source": [
    "# quest 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b87522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x)=w⋅x+b\n",
    "\n",
    "# Where:\n",
    "\n",
    "# 𝑤\n",
    "# w is the weight vector\n",
    "# 𝑥\n",
    "# x is the input feature vector\n",
    "# 𝑏\n",
    "# b is the bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c77208a",
   "metadata": {},
   "source": [
    "# quest 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af79a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The objective function of a linear Support Vector Machine (SVM) aims to maximize the margin between the decision boundary and the support vectors while minimizing the classification error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734dbae4",
   "metadata": {},
   "source": [
    "# quest 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f54f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "    #The kernel trick in Support Vector Machines (SVM) is a powerful technique that allows SVMs to efficiently handle non-linear classification tasks by implicitly mapping input features into a higher-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6556af4e",
   "metadata": {},
   "source": [
    "# quest 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac851c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Support vectors play a crucial role in Support Vector Machines (SVM). They are the data points that lie closest to the decision boundary (hyperplane) and have the most influence on determining the position and orientation of the boundary. Support vectors are critical because they directly influence the margin, which is the distance between the decision boundary and the nearest data point of any class.\n",
    "\n",
    "# Let's illustrate the role of support vectors with a simple example:\n",
    "\n",
    "# Imagine we have a binary classification problem where we aim to classify whether an email is spam (positive class) or not spam (negative class) based on two features: the number of words \"buy\" and \"discount\" in the email. We have the following data:\n",
    "\n",
    "# Positive class (spam): (3, 4), (4, 5), (5, 6)\n",
    "# Negative class (not spam): (1, 1), (2, 2), (3, 3)\n",
    "# Now, let's train a linear SVM on this data. The decision boundary will be a line that separates the positive and negative classes. However, only the data points closest to this decision boundary, i.e., the support vectors, are crucial for defining the boundary.\n",
    "\n",
    "# In this example, the support vectors are:\n",
    "\n",
    "# For the positive class (spam): (3, 4), (5, 6)\n",
    "# For the negative class (not spam): (1, 1), (3, 3)\n",
    "# These support vectors are the data points that lie closest to the decision boundary. They essentially define the margin of the SVM, which is the perpendicular distance from the decision boundary to the nearest support vector of any class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123ef2f",
   "metadata": {},
   "source": [
    "# quest 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b13a368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperplane:\n",
    "# In SVM, the hyperplane is the decision boundary that separates the data points of different classes in the feature space.\n",
    "# In a binary classification problem, the hyperplane is a (d-1)-dimensional subspace of the d-dimensional feature space.\n",
    "# Example: Consider a simple 2D dataset with two classes, denoted by red and blue points. The hyperplane is a line that separates these two classes.\n",
    "# Marginal Plane:\n",
    "# The marginal plane is the boundary parallel to the hyperplane, located at a distance of one margin from the hyperplane.\n",
    "# It defines the region where the support vectors lie.\n",
    "# Example: In the same 2D dataset, the marginal planes are lines parallel to the hyperplane and equidistant from it.\n",
    "# Hard Margin:\n",
    "# In a hard margin SVM, the decision boundary (hyperplane) is required to perfectly separate the classes without any misclassification.\n",
    "# It means no data points are allowed to fall within the margin.\n",
    "# Example: In the 2D dataset, a hard margin SVM would find a line that perfectly separates the red and blue points without any overlap.\n",
    "# Soft Margin:\n",
    "# In a soft margin SVM, a certain degree of misclassification is allowed to find a better generalization of the decision boundary, especially in cases where data is not perfectly separable.\n",
    "# Soft margin SVM introduces a penalty parameter (C) that controls the trade-off between maximizing the margin and minimizing the misclassification.\n",
    "# Example: In the 2D dataset, a soft margin SVM may allow some data points to fall within the margin or even on the wrong side of the decision boundary to achieve better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021b430",
   "metadata": {},
   "source": [
    "# quest 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffcde956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we'll load the Iris dataset, split it into training and testing sets, and train a linear SVM classifier using scikit-learn. Then, we'll implement a linear SVM classifier from scratch, train it on the training set, and evaluate its performance on the testing set. Finally, we'll plot the decision boundaries of the trained models and observe the effect of different values of the regularization parameter \n",
    "# 𝐶\n",
    "# C on the performance.\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn import datasets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Load the Iris dataset\n",
    "# iris = datasets.load_iris()\n",
    "# X = iris.data[:, :2]  # Using only the first two features for visualization\n",
    "# y = iris.target\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Train linear SVM classifier using scikit-learn\n",
    "# svm_clf_sklearn = SVC(kernel='linear')\n",
    "# svm_clf_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# # Predict labels for the testing set\n",
    "# y_pred_sklearn = svm_clf_sklearn.predict(X_test)\n",
    "\n",
    "# # Compute accuracy of the model\n",
    "# accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "# print(\"Accuracy of scikit-learn SVM:\", accuracy_sklearn)\n",
    "\n",
    "# # Plot decision boundaries for scikit-learn SVM\n",
    "# def plot_decision_boundary(clf, X, y):\n",
    "#     x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "#     y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "#     xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "#                          np.arange(y_min, y_max, 0.01))\n",
    "#     Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "#     plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "#     plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "#     plt.xlabel('Sepal length')\n",
    "#     plt.ylabel('Sepal width')\n",
    "#     plt.title('Decision Boundary (scikit-learn SVM)')\n",
    "#     plt.show()\n",
    "\n",
    "# plot_decision_boundary(svm_clf_sklearn, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's implement a linear SVM classifier from scratch and compare its performance with scikit-learn's implementation.\n",
    "\n",
    "class LinearSVM:\n",
    "    def __init__(self, lr=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * 1 / self.n_iters * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * 1 / self.n_iters * self.w - np.dot(x_i, y[idx]))\n",
    "                    self.b -= self.lr * y[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.w) - self.b\n",
    "        return np.sign(linear_output)\n",
    "\n",
    "# Train linear SVM classifier from scratch\n",
    "svm_clf_scratch = LinearSVM()\n",
    "svm_clf_scratch.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred_scratch = svm_clf_scratch.predict(X_test)\n",
    "\n",
    "# Compute accuracy of the model\n",
    "accuracy_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "print(\"Accuracy of scratch SVM:\", accuracy_scratch)\n",
    "\n",
    "# Plot decision boundaries for scratch SVM\n",
    "plot_decision_boundary(svm_clf_scratch, X_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
